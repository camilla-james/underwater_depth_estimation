{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "# external imports\n",
    "import transformers\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import torch\n",
    "import torchvision\n",
    "import time \n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont  # Import ImageDraw and ImageFont\n",
    "import requests\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Intial Tests with DepthAnything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Investigating FLSea Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_labels(predicted_labels, ground_truth_labels):\n",
    "    # Calculate mean or median values\n",
    "    predicted_mean = np.mean(predicted_labels)\n",
    "    ground_truth_mean = np.mean(ground_truth_labels)\n",
    "\n",
    "    # Calculate the shift\n",
    "    shift = ground_truth_mean - predicted_mean\n",
    "\n",
    "    # Adjust the predicted labels\n",
    "    shifted_predicted_labels = predicted_labels + shift\n",
    "\n",
    "    return shifted_predicted_labels\n",
    "\n",
    "\n",
    "def plot_histograms(groundtruth, predicted):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(groundtruth, bins=50, alpha=0.5, color='blue', label='ground truth')\n",
    "    plt.hist(predicted, bins=50, alpha=0.5, color='red', label='predicted')\n",
    "    plt.title('Histogram of Depth Variables')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def min_max_normalize(image):\n",
    "    # Get the minimum and maximum pixel values\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    if min_val == max_val:\n",
    "        return np.empty((0,))\n",
    "    # Normalize the image\n",
    "    normalized_image = (image - min_val) / (max_val - min_val)\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_correction(image, gamma):\n",
    "    \"\"\"\n",
    "    Apply gamma correction to the input jpg image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert PIL image to numpy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Apply gamma correction\n",
    "    corrected_image_array = np.uint8(255 * (image_array / 255) ** gamma)\n",
    "    \n",
    "    # Ensure no negative values\n",
    "    min_value = np.min(corrected_image_array)\n",
    "    corrected_image_array_shifted = corrected_image_array - min_value\n",
    "        \n",
    "    # Convert back to PIL image\n",
    "    # corrected_image = Image.fromarray(corrected_image_array_shifted)\n",
    "    \n",
    "    return corrected_image_array_shifted\n",
    "\n",
    "\n",
    "class GammaCorrectionAndNormalization(torch.nn.Module):\n",
    "    def __init__(self, gamma, stretch_factor):\n",
    "        super(GammaCorrectionAndNormalization, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.stretch_factor = stretch_factor\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Apply gamma correction\n",
    "        gamma_corrected_img = gamma_correction(img, gamma=self.gamma)\n",
    "\n",
    "        # Convert gamma-corrected image to numpy array\n",
    "        # img_array = np.array(gamma_corrected_img)\n",
    "\n",
    "#         # Apply contrast stretching to each channel independently\n",
    "#         stretched_img_array = np.zeros_like(gamma_corrected_img)\n",
    "#         for i in range(gamma_corrected_img.shape[2]):  # Iterate over channels\n",
    "#             channel_min = np.min(gamma_corrected_img[:, :, i])\n",
    "#             channel_max = np.max(gamma_corrected_img[:, :, i])\n",
    "#             stretched_img_array[:, :, i] = (gamma_corrected_img[:, :, i] - channel_min) * self.stretch_factor / (channel_max - channel_min)\n",
    "\n",
    "#         # Calculate mean and standard deviation\n",
    "#         mean_values = np.mean(gamma_corrected_img, axis=(0, 1))\n",
    "#         std_values = np.std(gamma_corrected_img, axis=(0, 1))\n",
    "        \n",
    "        # Normalize the image using calculated mean and standard deviation\n",
    "        # normalized_img_array = (gamma_corrected_img - mean_values) / std_values\n",
    "        # Convert numpy array back to PIL image\n",
    "        # normalized_img = Image.fromarray((normalized_img_array).astype(np.uint8))\n",
    "\n",
    "        # Min-max normalization\n",
    "        min_value = np.min(gamma_corrected_img)\n",
    "        max_value = np.max(gamma_corrected_img)\n",
    "        # print(min_value, max_value)\n",
    "        normalized_img_array = (gamma_corrected_img - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "        return normalized_img_array\n",
    "    \n",
    "    \n",
    "class MinMaxScaler:\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.min_val = torch.tensor(min_val, dtype=torch.float32)\n",
    "        self.max_val = torch.tensor(max_val, dtype=torch.float32)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Perform Min-Max scaling on the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "            tensor (Tensor): Input tensor to be scaled.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scaled tensor in the range [0, 1].\n",
    "        \"\"\"\n",
    "        # Perform Min-Max scaling\n",
    "\n",
    "        # scaled_tensor = (tensor - self.min_val) / (self.max_val - self.min_val)\n",
    "        \n",
    "        # Scale each channel independently\n",
    "        scaled_tensor = tensor.clone()  # Create a copy of the input tensor\n",
    "        for i in range(3):  # Assuming RGB channels\n",
    "            scaled_tensor[i] = (tensor[i] - self.min_val[i]) / (self.max_val[i] - self.min_val[i])\n",
    "            \n",
    "        # Clip values to ensure they are within [0, 1] range\n",
    "        scaled_tensor = torch.clamp(scaled_tensor, 0, 1)\n",
    "\n",
    "        return scaled_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDepthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, depth_dir, transform_img=None, transform_depth=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_depth = transform_depth\n",
    "\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "        self.depth_filenames = os.listdir(depth_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def mask_edges(self, image, threshold1=50, threshold2=110, dilation_kernel_size=3):\n",
    "        # Convert the image to grayscale\n",
    "        gray = image.convert('L')\n",
    "\n",
    "        # Convert grayscale image to numpy array\n",
    "        gray_np = np.array(gray)\n",
    "\n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(gray_np, threshold1=threshold1, threshold2=threshold2)\n",
    "\n",
    "        # Create a dilation kernel\n",
    "        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n",
    "\n",
    "        # Dilate the edges to create a thicker boundary\n",
    "        dilated_edges = cv2.dilate(edges, kernel)\n",
    "\n",
    "        # Create a mask for pixels near edges\n",
    "        mask = dilated_edges != 0\n",
    "\n",
    "        # Apply the mask to the depth image\n",
    "        depth_array = np.array(image)\n",
    "        depth_array[mask] = 0\n",
    "\n",
    "        return Image.fromarray(depth_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        depth_name = os.path.join(self.depth_dir, self.depth_filenames[idx])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "        depth = Image.open(depth_name)\n",
    "\n",
    "        # Apply edge mask to the depth image\n",
    "        depth = self.mask_edges(depth)\n",
    "\n",
    "        if self.transform_img:\n",
    "            image = self.transform_img(image)\n",
    "        if self.transform_depth:\n",
    "            depth = self.transform_depth(depth)\n",
    "\n",
    "        return image, depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'C:\\\\Users\\\\susan\\\\Documents\\\\University\\\\MIR\\\\DataDriven\\\\FLSea\\\\canyons\\\\tiny_canyon\\\\tiny_canyon\\\\imgs\\\\'\n",
    "depth_path = 'C:\\\\Users\\\\susan\\\\Documents\\\\University\\\\MIR\\\\DataDriven\\\\FLSea\\\\canyons\\\\tiny_canyon\\\\tiny_canyon\\\\depth\\\\'\n",
    "\n",
    "\n",
    "# Define gamma value\n",
    "gamma = 0.7\n",
    "stretch_factor = 0.5\n",
    "# mean_values_norm = [0.485, 0.456, 0.406]\n",
    "# std_values_norm =  [0.229, 0.224, 0.225]\n",
    "\n",
    "mean_values_norm = [0.700, 0.600, 0.200]\n",
    "std_values_norm =  [0.5, 0.224, 0.1]\n",
    "\n",
    "min_val = [-1.4000, -2.6786, -1.8800]\n",
    "max_val = [0.6000, 1.7857, 8.0000]\n",
    "# Create the custom transform\n",
    "custom_transform = GammaCorrectionAndNormalization(gamma, stretch_factor)\n",
    "scaler = MinMaxScaler(min_val, max_val)\n",
    "\n",
    "\n",
    "transform_img = transforms.Compose([\n",
    "    custom_transform,\n",
    "    # transforms.Lambda(lambda x: gamma_correction(x, gamma=0.7)), # Apply gamma correction\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor \n",
    "    transforms.Normalize(mean_values_norm, std_values_norm),  # Normalize the tensor\n",
    "    scaler, # Min-Max scaling  \n",
    "])\n",
    "\n",
    "\n",
    "transform_depth = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create Dataset\n",
    "train_data = CustomDepthDataset(image_path, depth_path, transform_img=transform_img, transform_depth=transform_depth)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4\n",
    "dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Histogramize_dataloader(dataloader):\n",
    "    # Initialize empty lists to store pixel values for each channel\n",
    "    pixel_values_R = []\n",
    "    pixel_values_G = []\n",
    "    pixel_values_B = []\n",
    "\n",
    "    # Iterate over the dataset using the DataLoader\n",
    "    for images, depths in tqdm(dataloader, desc=\"Getting RGB values...\", unit=\"batch\") :\n",
    "\n",
    "\n",
    "        # Iterate over each image in the batch\n",
    "        for image in images:\n",
    "            # Split the image into channels\n",
    "            R, G, B = image[0].cpu().numpy(), image[1].cpu().numpy(), image[2].cpu().numpy()\n",
    "    \n",
    "\n",
    "    # Plot histograms for each channel\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "\n",
    "    # Plot histogram for the R channel\n",
    "    axs[0].hist(np.ravel(R),  color='red')\n",
    "    axs[0].set_xlabel('Pixel Value')\n",
    "    axs[0].set_ylabel('Frequency')\n",
    "    axs[0].set_title('Histogram of Red Channel')\n",
    "\n",
    "    # Plot histogram for the G channel\n",
    "    axs[1].hist(np.ravel(G),  color='green')\n",
    "    axs[1].set_xlabel('Pixel Value')\n",
    "    axs[1].set_ylabel('Frequency')\n",
    "    axs[1].set_title('Histogram of Green Channel')\n",
    "\n",
    "    # Plot histogram for the B channel\n",
    "    axs[2].hist(np.ravel(B), color='blue')\n",
    "    axs[2].set_xlabel('Pixel Value')\n",
    "    axs[2].set_ylabel('Frequency')\n",
    "    axs[2].set_title('Histogram of Blue Channel')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Histogramize_dataloader(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate min and max values\n",
    "min_values = torch.ones(3)\n",
    "max_values = torch.zeros(3)\n",
    "total_samples = 0\n",
    "\n",
    "global_min = torch.ones(3)\n",
    "global_max = torch.zeros(3)\n",
    "\n",
    "for batch_imgs, _ in tqdm(dataloader, desc=\"Computing min max\", unit=\"batch\"):\n",
    "    \n",
    "    # Calculate the minimum value for each channel\n",
    "    batch_min = torch.min(batch_imgs, dim=2, keepdim=True)[0]  # Take the min across height dimension\n",
    "    \n",
    "    batch_min = torch.min(batch_min, dim=3)[0]  # Take the min across width dimension\n",
    "    \n",
    "    batch_max = torch.max(batch_imgs, dim=2, keepdim=True)[0]  # Take the max across height dimension\n",
    "    batch_max = torch.max(batch_max, dim=3)[0]  # Take the max across width dimension\n",
    "\n",
    "    # find min and max in the current batch\n",
    "    min_values = torch.min(batch_min, dim=0)[0].squeeze()\n",
    "    max_values = torch.max(batch_max, dim=0)[0].squeeze()\n",
    "\n",
    "    # update the global min/max them if necessary\n",
    "    global_min =  torch.where(min_values < global_min, min_values, global_min)\n",
    "    global_max =  torch.where(max_values > global_max, max_values, global_max)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Min values:\", global_min)\n",
    "print(\"Max values:\", global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean_values = torch.zeros(3)\n",
    "std_values = torch.zeros(3)\n",
    "total_samples = 0\n",
    "\n",
    "\n",
    "for batch_imgs, _ in tqdm(dataloader, desc=\"Computing mean and standard deviation\", unit=\"batch\"):\n",
    "    # Calculate mean and std per batch\n",
    "    batch_mean = torch.mean(batch_imgs, dim=(0, 2, 3))\n",
    "    batch_std = torch.std(batch_imgs, dim=(0, 2, 3))\n",
    "    \n",
    "    # Update total mean and std\n",
    "    mean_values += batch_mean\n",
    "    std_values += batch_std\n",
    "    \n",
    "    # Update total number of samples\n",
    "    total_samples += batch_imgs.size(0)\n",
    "\n",
    "# Calculate final mean and std\n",
    "mean_values /= total_samples\n",
    "std_values /= total_samples\n",
    "\n",
    "print(\"Mean values:\", mean_values)\n",
    "print(\"Standard deviation values:\", std_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample depths from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_depth(dataloader, k, model, image_processor):\n",
    "    images_container = []\n",
    "    depths_container = []\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Create a grid to display the images\n",
    "    fig, axes = plt.subplots(k, 3, figsize=(10, 2*k))\n",
    "\n",
    "    # Iterate over batches in the DataLoader\n",
    "    for images, depths in dataloader:\n",
    "        \n",
    "        i +=1\n",
    "        \n",
    "        # Stop if processed k batches\n",
    "        if i >= k:\n",
    "            break\n",
    "  \n",
    "        # Prepare images for the model\n",
    "        inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model\n",
    "            outputs = model(**inputs)\n",
    "            predicted_depths = outputs.predicted_depth\n",
    "\n",
    "        # Iterate over images in the batch\n",
    "        for j in range(images.shape[0]):\n",
    "            # Interpolate to original size\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                predicted_depths[j].unsqueeze(0).unsqueeze(0),\n",
    "                size=[images.shape[2], images.shape[3]],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            # Convert depth prediction to numpy array\n",
    "            depth_output = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "            # Plot original image\n",
    "            axes[j, 0].imshow(images[j].permute(1, 2, 0))\n",
    "            axes[j, 0].axis('off')\n",
    "            axes[j, 0].set_title(f'Image {i*k + j}')\n",
    "\n",
    "            # Plot depth output\n",
    "            axes[j, 1].imshow(depth_output, cmap='plasma')\n",
    "            axes[j, 1].axis('off')\n",
    "            axes[j, 1].set_title(f'Depth {i*k + j}')\n",
    "\n",
    "            # Plot ground truth depth\n",
    "            axes[j, 2].imshow(depths[j].squeeze(), cmap='plasma_r')\n",
    "            axes[j, 2].axis('off')\n",
    "            axes[j, 2].set_title(f'Ground Truth Depth {i*k + j}')\n",
    "\n",
    "\n",
    "            images_container.append(images[j].permute(1, 2, 0).numpy())\n",
    "            depths_container.append(depths[j].numpy())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return np.asarray(images_container), np.asarray(depths_container)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Results -> No training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement some basic metrics for getting baseline results for the entire dataset. DepthAnything used these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_relative_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Relative Error (MARE).\n",
    "\n",
    "    Parameters:\n",
    "    y_pred : torch.Tensor\n",
    "        Predicted depth values.\n",
    "    y_true : torch.Tensor\n",
    "        Ground truth depth values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Mean Absolute Relative Error (MARE).\n",
    "    \"\"\"\n",
    "    # Mask out both zeros and NaNs\n",
    "    mask = (y_true != 0) & (~np.isnan(y_true))\n",
    "    \n",
    "    # Apply the mask to both predicted and true values\n",
    "    y_pred_masked = y_pred[mask]\n",
    "    y_true_masked = y_true[mask]\n",
    "\n",
    "    # Calculate absolute relative error\n",
    "    absolute_relative_error = np.abs(y_pred_masked - y_true_masked) / y_true_masked\n",
    "    \n",
    "    # Calculate mean excluding masked values\n",
    "    mare = np.mean(absolute_relative_error)\n",
    "    \n",
    "    return mare.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_pred, y_true): \n",
    "    mse = np.mean((y_pred - y_true)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta1_metric(y_pred, y_true, threshold=1.25):\n",
    "    \"\"\"\n",
    "    Calculate the δ1 metric for monocular depth estimation.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred : torch.Tensor\n",
    "        Predicted depth values.\n",
    "    y_true : torch.Tensor\n",
    "        Ground truth depth values.\n",
    "    threshold : float, optional\n",
    "        Threshold for considering a pixel as correctly estimated (default is 1.25).\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Percentage of pixels for which max(d*/d, d/d*) < threshold.\n",
    "    \"\"\"\n",
    "\n",
    "     # Mask out both zeros and NaNs\n",
    "    # nan_mask = np.isnan(y_true) | np.isnan(y_pred) | (y_true == 0)\n",
    "    \n",
    "    # Apply the mask to both predicted and true values\n",
    "    # y_pred_masked = y_pred[~nan_mask]\n",
    "    # y_true_masked = y_true[~nan_mask]\n",
    "    \n",
    "    # Calculate element-wise ratios\n",
    "    ratio_1 = y_true / (y_pred + 1e-7)  # Adding epsilon to avoid division by zero\n",
    "    ratio_2 = y_pred / (y_true + 1e-7) # Adding epsilon to avoid division by zero\n",
    "    \n",
    "    # Calculate element-wise maximum ratio\n",
    "    max_ratio = np.maximum(ratio_1, ratio_2)\n",
    "\n",
    "    # print(max_ratio)\n",
    "    # Count the number of pixels where max_ratio < threshold\n",
    "    num_correct_pixels = np.sum(max_ratio < threshold)\n",
    "    # print(\"num correct pixels\", num_correct_pixels)\n",
    "    \n",
    "    # Calculate the percentage of pixels satisfying the condition\n",
    "    total_pixels = len(y_true)\n",
    "    # print(\"num total_pixels\",total_pixels)\n",
    "    ratio_correct = num_correct_pixels / total_pixels\n",
    "    return ratio_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_edges(image, threshold1=50, threshold2=110, dilation_kernel_size=10):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, threshold1=threshold1, threshold2=threshold2)\n",
    "\n",
    "    # Create a dilation kernel\n",
    "    kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n",
    "\n",
    "    # Dilate the edges to create a thicker boundary\n",
    "    dilated_edges = cv2.dilate(edges, kernel)\n",
    "\n",
    "    # Invert the dilated edges mask\n",
    "    dilated_edges = cv2.bitwise_not(dilated_edges)\n",
    "\n",
    "    # Create a black background image\n",
    "    background = np.zeros_like(image)\n",
    "\n",
    "    # Copy the original image where the edges are not detected\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=dilated_edges)\n",
    "\n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now get the depths for the whole dataset, and compute baseline results for MAE and delta1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/main/en/preprocessing#computer-vision\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    mae = 0\n",
    "    delta1 = 0\n",
    "    total = 0\n",
    "    rmse = 0\n",
    "    num_batch = 0\n",
    "    skipped = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    for data, labels in tqdm(dataloader, desc=\"Computing metrics \", unit=\"batch\"):\n",
    "        inputs = image_processor(images=data, return_tensors=\"pt\", do_rescale= False).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "            # interpolate to original size\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                predicted_depth.unsqueeze(1),\n",
    "                size=[data.shape[2], data.shape[3]],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            # calculate prediction\n",
    "            output = prediction.squeeze().cpu().numpy()\n",
    "            if output.size == 0:\n",
    "                print(\"Skipping image\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                normalized_out = min_max_normalize(output)\n",
    "                if normalized_out.size == 0:\n",
    "                    print(\"Skipping image\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            \n",
    "            labels_normalized = min_max_normalize(labels.squeeze().numpy())\n",
    "            if labels_normalized.size == 0:\n",
    "                print(\"Skipping image\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "           \n",
    "            # Create a mask for non-zero values in labels\n",
    "            mask = labels_normalized != 0\n",
    "            # Apply the mask to both predictions and labels\n",
    "            normalized_out_masked = normalized_out[mask]\n",
    "            labels_normalized_masked = labels_normalized[mask]\n",
    "            \n",
    "             # Create a mask for non-zero values in predictions\n",
    "            mask2 = normalized_out_masked != 0\n",
    "            # Apply the mask to both predictions and labels\n",
    "            normalized_out_masked2 = normalized_out_masked[mask2]\n",
    "            labels_normalized_masked2 = labels_normalized_masked[mask2]\n",
    "            \n",
    "            if labels_normalized_masked.size == 0:\n",
    "                print(\"Skipping image\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "                                    \n",
    "#             final_outputs = normalized_out_masked\n",
    "#             final_labels = labels_normalized_masked\n",
    "\n",
    "            shifted_predicted_labels = shift_labels(normalized_out_masked, labels_normalized_masked)\n",
    "            # mask_outliers = shifted_predicted_labels <= 1\n",
    "            \n",
    "            final_outputs = shifted_predicted_labels #[mask_outliers]\n",
    "            final_labels = labels_normalized_masked #[mask_outliers]\n",
    "            \n",
    "            # Plot histograms after shifting\n",
    "            # plot_histograms(final_labels, final_outputs)\n",
    "\n",
    "            # print(\"shape masked labels\", labels_normalized_masked.shape)\n",
    "            # print(labels_normalized_masked)\n",
    "            \n",
    "            single_mae = absolute_relative_error(final_outputs, final_labels)\n",
    "            if math.isnan(single_mae):\n",
    "                print(\"Skipping image\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            mae += single_mae\n",
    "\n",
    "            rmse += root_mean_squared_error(final_outputs, final_labels)\n",
    "            # print(\"rmse\", root_mean_squared_error(normalized_out_masked, labels_normalized_masked))\n",
    "            \n",
    "            delta1 += delta1_metric(final_outputs, final_labels, 1.25)\n",
    "            # print(\"delta1\", delta1_metric(normalized_out_masked, labels_normalized_masked))\n",
    "            num_batch += 1\n",
    "            # print(\"MAE \", mae/num_batch, \", RMSE \", rmse/num_batch, \", delta \", delta1/num_batch)\n",
    "\n",
    "\n",
    "    print(\"total\", num_batch)\n",
    "    print(\"mae\", mae)\n",
    "    print(\"delta1\", delta1)\n",
    "    print(\"rmse\", rmse)\n",
    "    total_mae = mae/num_batch\n",
    "    total_delta = delta1/num_batch\n",
    "    total_rmse = rmse/num_batch\n",
    "    \n",
    "\n",
    "    return total_mae, total_delta, total_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = 'C:\\\\Users\\\\susan\\\\Documents\\\\University\\\\MIR\\\\DataDriven\\\\FLSea\\\\canyons\\\\tiny_canyon\\\\tiny_canyon\\\\imgs\\\\'\n",
    "depth_path = 'C:\\\\Users\\\\susan\\\\Documents\\\\University\\\\MIR\\\\DataDriven\\\\FLSea\\\\canyons\\\\tiny_canyon\\\\tiny_canyon\\\\depth\\\\'\n",
    "\n",
    "\n",
    "# Define gamma value\n",
    "gamma = 0.7\n",
    "stretch_factor = 0.5\n",
    "\n",
    "mean_values_norm = [0.700, 0.600, 0.200]\n",
    "std_values_norm = [0.5, 0.224, 0.1]\n",
    "\n",
    "min_val = [-1.4000, -2.6786, -1.8800]\n",
    "max_val = [0.6000, 1.7857, 8.0000]\n",
    "# Create the custom transform\n",
    "custom_transform = GammaCorrectionAndNormalization(gamma, stretch_factor)\n",
    "scaler = MinMaxScaler(min_val, max_val)\n",
    "\n",
    "\n",
    "transform_img = transforms.Compose([\n",
    "    custom_transform,\n",
    "    # transforms.Lambda(lambda x: gamma_correction(x, gamma=0.7)), # Apply gamma correction\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor \n",
    "    transforms.Normalize(mean_values_norm, std_values_norm),  # Normalize the tensor\n",
    "    scaler, # Min-Max scaling  \n",
    "])\n",
    "\n",
    "\n",
    "transform_depth = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create Dataset\n",
    "train_data = CustomDepthDataset(image_path, depth_path, transform_img=transform_img, transform_depth=transform_depth)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics :  94%|█████████████████████████████████████████████████████▊   | 239/253 [11:16<00:35,  2.53s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing metrics :  95%|██████████████████████████████████████████████████████   | 240/253 [11:19<00:32,  2.48s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing metrics :  95%|██████████████████████████████████████████████████████▎  | 241/253 [11:21<00:29,  2.49s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Computing metrics :  96%|██████████████████████████████████████████████████████▌  | 242/253 [11:24<00:27,  2.52s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics : 100%|█████████████████████████████████████████████████████████| 253/253 [11:54<00:00,  2.82s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 249\n",
      "mae 180.81666080653667\n",
      "delta1 60.8162394238533\n",
      "rmse 86.1224811822176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.726171328540308, 0.24424192539700118, 0.3458734184024803)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "result = evaluate_model(model, train_loader, device)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "result_large = evaluate_model(model, train_loader, device)\n",
    "result_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nopre = (0.6780142457071079, 0.25126540302579914, 0.33210749553149965)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_withpre = (0.726171328540308, 0.24424192539700118, 0.3458734184024803)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Delta1</th>\n",
       "      <th>AbsRel</th>\n",
       "      <th>AbsRel Log</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RMSE Log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DepthAnything-Small: No edge-detection</td>\n",
       "      <td>0.251265</td>\n",
       "      <td>0.678014</td>\n",
       "      <td>-0.168761</td>\n",
       "      <td>0.332107</td>\n",
       "      <td>-0.478721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DepthAnything-Small: With edge-detection</td>\n",
       "      <td>0.244242</td>\n",
       "      <td>0.726171</td>\n",
       "      <td>-0.138961</td>\n",
       "      <td>0.345873</td>\n",
       "      <td>-0.461083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Model    Delta1    AbsRel  AbsRel Log  \\\n",
       "0    DepthAnything-Small: No edge-detection  0.251265  0.678014   -0.168761   \n",
       "1  DepthAnything-Small: With edge-detection  0.244242  0.726171   -0.138961   \n",
       "\n",
       "       RMSE  RMSE Log  \n",
       "0  0.332107 -0.478721  \n",
       "1  0.345873 -0.461083  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas to use pandas DataFrame\n",
    "log10_mae_nopre = np.log10(results_nopre[0])\n",
    "log10_rmse_nopre = np.log10(results_nopre[2])\n",
    "\n",
    "log10_mae_withpre = np.log10(results_withpre[0])\n",
    "log10_rmse_withpre = np.log10(results_withpre[2])\n",
    "\n",
    "nopre = (\"DepthAnything-Small: No edge-detection\", results_nopre[1], results_nopre[0], log10_mae_nopre, results_nopre[2], log10_rmse_nopre)\n",
    "withpre = (\"DepthAnything-Small: With edge-detection\", results_withpre[1], results_withpre[0], log10_mae_withpre, results_withpre[2], log10_rmse_withpre)\n",
    "\n",
    "# data in the form of list of tuples\n",
    "data = [nopre, withpre]\n",
    " \n",
    "# create DataFrame using data\n",
    "df = pd.DataFrame(data, columns =['Model', 'Delta1', 'AbsRel', 'AbsRel Log', 'RMSE', 'RMSE Log'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_enhanced = \"C:\\\\Users\\\\susan\\\\Documents\\\\University\\\\MIR\\\\DataDriven\\\\FLSea\\\\canyons\\\\tiny_canyon\\\\tiny_canyon\\\\seaErra\"\n",
    "# Load the data\n",
    "train_data_enhanced = SegmentationDataset(image_path_enhanced, depth_path, transforms=data_transforms)\n",
    "f\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_data_enhanced, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "result = evaluate_model(model, train_loader, device)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
