{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m/public/conda/user_envs/cjames706/envs/depth_estimation/lib/python3.12/site-packages/matplotlib/pyplot.py:66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _docstring\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m     FigureCanvasBase, FigureManagerBase, MouseButton)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Figure, FigureBase, figaspect\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcsetup, rcParamsDefault, rcParamsOrig\n",
      "File \u001b[0;32m/public/conda/user_envs/cjames706/envs/depth_estimation/lib/python3.12/site-packages/matplotlib/figure.py:43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _blocking_input, backend_bases, _docstring, projections\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     DrawEvent, FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "File \u001b[0;32m/public/conda/user_envs/cjames706/envs/depth_estimation/lib/python3.12/site-packages/matplotlib/projections/__init__.py:55\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m axes, _docstring\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolarAxes\n",
      "File \u001b[0;32m/public/conda/user_envs/cjames706/envs/depth_estimation/lib/python3.12/site-packages/matplotlib/axes/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _base\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_axes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Backcompat.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_axes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes \u001b[38;5;28;01mas\u001b[39;00m Subplot\n",
      "File \u001b[0;32m/public/conda/user_envs/cjames706/envs/depth_estimation/lib/python3.12/site-packages/matplotlib/axes/_axes.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpath\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquiver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmquiver\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstackplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmstack\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreamplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmstream\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmtable\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:990\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1086\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "# external imports\n",
    "import transformers\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import torch\n",
    "import torchvision\n",
    "import time \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial Tests with DepthAnything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DepthEstimation Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)\n",
    "depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Budget Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset -> stream it so it does not take too long\n",
    "train_dataset = load_dataset(\"sayakpaul/nyu_depth_v2\", split=\"train\", \n",
    "                                streaming = True, trust_remote_code=True)\n",
    "\n",
    "# load the dataset into a dataloader\n",
    "dataset = train_dataset.with_format(\"torch\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=2, batch_size = 16)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "# prepare image for the model\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    t0 = time.time()\n",
    "    image = batch.get('image')\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size= [image.size()[1], image.size()[2]],#image.size(),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # visualize the prediction\n",
    "    # output = prediction.squeeze().cpu().numpy()\n",
    "    # print(output)\n",
    "    # formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "    # depth = Image.fromarray(formatted)\n",
    "    # plt.imshow(depth)\n",
    "    print(f\"The time taken is:{time.time()-t0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the small model, processing batches of size 16 took about 0.5 seconds on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Investigating Eiffel Tower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'eiffel/2020/' #this is the path to the 2020 images\n",
    "depth_path = 'eiffel/2020/depth/dense/' # this is the path to 2020 ground truth depths\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(image_path, transform = transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers = 2, batch_size = 16)\n",
    "sample = next(iter(dataloader))[0].numpy() # get one image\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling one image and getting the depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(sample)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = image_processor(images=sample, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# interpolate to original size \n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=[sample.shape[1], sample.shape[2]],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "# visualize the prediction\n",
    "output = prediction.squeeze().cpu().numpy()\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)\n",
    "depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a depth map! But it does not say much. Let's process a batch, and compare the images and the found depth map with the ground truth. Below is code from Madhu's test file that I plan to alter to fit our image files -> should not take too long. \n",
    "Then I will process the whole dataset and get some metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(image):\n",
    "    # Get the minimum and maximum pixel values\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalized_image = (image - min_val) / (max_val - min_val)\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_eiffel_depth(image_path, depth_path, k, model, image_processor):\n",
    "    # Get a list of all files in the directory\n",
    "    image_files = os.listdir(image_path)\n",
    "\n",
    "    # Shuffle the list of image files\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Select k random images\n",
    "    selected_images = image_files[:k]\n",
    "\n",
    "    # Create a grid to display the images\n",
    "    fig, axes = plt.subplots(k, 3, figsize=(10, 2*k))\n",
    "\n",
    "    for i, img_file in enumerate(selected_images):\n",
    "        # Load the original image\n",
    "        img_path = os.path.join(image_path, img_file)\n",
    "        assert os.path.exists(img_path)\n",
    "\n",
    "        original_image = Image.open(img_path)\n",
    "        original_image_display = cv2.imread(img_path)\n",
    "\n",
    "        img_lab = cv2.cvtColor(original_image_display,cv2.COLOR_BGR2Lab)\n",
    "        l,a,b = cv2.split(img_lab)\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=50, tileGridSize=(8, 8))\n",
    "        lumen_clahe = clahe.apply(l)\n",
    "        equ = cv2.equalizeHist(l)\n",
    "\n",
    "        updated_lab_img = cv2.merge((lumen_clahe,a,b))\n",
    "\n",
    "        original_image_display = cv2.cvtColor(updated_lab_img,cv2.COLOR_LAB2LBGR)\n",
    "\n",
    "        # Prepare image for the model\n",
    "        inputs = image_processor(images=original_image, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model\n",
    "            outputs = model(**inputs)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "\n",
    "        # Interpolate to original size\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            predicted_depth.unsqueeze(1),\n",
    "            size=[np.array(original_image).shape[0], np.array(original_image).shape[1]],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        # Convert depth prediction to numpy array\n",
    "        depth_output = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(original_image)\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 0].set_title(f'Image{i}',)\n",
    "\n",
    "        # Plot depth output\n",
    "        axes[i, 1].imshow(depth_output,cmap = 'plasma')\n",
    "        axes[i, 1].axis('off')\n",
    "        axes[i, 1].set_title(f'Depth{i}')\n",
    "\n",
    "        # Load ground truth depth from TIFF file\n",
    "        depth_file = 'depth_' + img_file\n",
    "        depths = os.path.join(depth_path, depth_file)\n",
    "        depth_image = Image.open(depths)\n",
    "        depth_array = np.array(depth_image)\n",
    "        depth_normalized = min_max_normalize(depth_array)\n",
    "\n",
    "\n",
    "        # Plot ground truth depth\n",
    "        axes[i, 2].imshow(depth_normalized, cmap='plasma_r')\n",
    "        axes[i, 2].axis('off')\n",
    "        axes[i, 2].set_title(f'Ground Truth Depth {i}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'eiffel/2020/images/'\n",
    "depth_path = 'eiffel/2020/depth/dense/depth'\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "sample_eiffel_depth(image_path, depth_path, 10, model, image_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Results -> No training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement some basic metrics for getting baseline results for the entire dataset. DepthAnything used these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scale_offset(y_pred, y_true):\n",
    "        scale_factor = np.mean(y_pred) / np.mean(y_true)\n",
    "\n",
    "        # Adjust the second depth map by the scale factor\n",
    "        true_scaled = y_pred * scale_factor\n",
    "\n",
    "         # Calculate the offset\n",
    "        offset = np.mean(y_pred) - (scale_factor * np.mean(y_true))\n",
    "\n",
    "        # Adjust the second depth map by the offset\n",
    "        true_adjusted = true_scaled + offset\n",
    "\n",
    "        val_min = y_pred.min()\n",
    "        val_range = y_pred.max() - val_min + 1e-7\n",
    "\n",
    "        pred_normed = (y_pred - val_min) / val_range\n",
    "\n",
    "        # apply identical normalization to the denoised image (important!)\n",
    "        true_adjusted_normed = (true_adjusted - val_min) / val_range\n",
    "\n",
    "        return pred_normed, true_adjusted_normed\n",
    "\n",
    "    def absolute_relative_error(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculate the Absolute Relative Error (MARE).\n",
    "\n",
    "        Parameters:\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted depth values.\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth depth values.\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "        Absolute Relative Error (MARE).\n",
    "        \"\"\"\n",
    "        y_pred, y_true = scale_offset(y_pred, y_true)\n",
    "        # mask = y_true == 0\n",
    "        # y_true[mask] = 1 \n",
    "        absolute_relative_error = np.abs(y_pred - y_true) / y_true\n",
    "\n",
    "        return np.mean(absolute_relative_error)\n",
    "    \n",
    "    def root_mean_squared_error(y_pred, y_true, log = False): \n",
    "        \n",
    "        y_pred, y_true = scale_offset(y_pred, y_true)\n",
    "        if log:\n",
    "            mask = y_pred > 0.00001\n",
    "            y_pred = y_pred[mask]\n",
    "            y_true = y_true[mask]\n",
    "            y_pred = np.log(y_pred)\n",
    "            y_true = np.log(y_true)\n",
    "        mse = np.mean((y_pred - y_true)**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    def delta1_metric(y_pred, y_true, threshold=1.25):\n",
    "        \"\"\"\n",
    "        Calculate the Œ¥1 metric for monocular depth estimation.\n",
    "\n",
    "        Parameters:\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted depth values.\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth depth values.\n",
    "        threshold : float, optional\n",
    "            Threshold for considering a pixel as correctly estimated (default is 1.25).\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Percentage of pixels for which max(d*/d, d/d*) < threshold.\n",
    "            \n",
    "        \"\"\"\n",
    "        y_pred, y_true = scale_offset(y_pred.numpy(), y_true.numpy())\n",
    "        y_pred = torch.from_numpy(y_pred)\n",
    "        y_true = torch.from_numpy(y_true)\n",
    "        # Compute element-wise ratios\n",
    "        ratio_1 = y_true / (y_pred + 1e-7)  # Adding epsilon to avoid division by zero\n",
    "        ratio_2 = (y_pred + 1e-7) / y_true  # Adding epsilon to avoid division by zero\n",
    "        \n",
    "        # Calculate element-wise maximum ratio\n",
    "        max_ratio = torch.max(ratio_1, ratio_2)\n",
    "        \n",
    "        # Count the number of pixels where max_ratio < threshold\n",
    "        num_correct_pixels = torch.sum(max_ratio < threshold).item()\n",
    "        \n",
    "        # Calculate the percentage of pixels satisfying the condition\n",
    "        total_pixels = y_true.numel()\n",
    "        percentage_correct = (num_correct_pixels / total_pixels) * 100.0\n",
    "        \n",
    "        return percentage_correct\n",
    "    \n",
    "    def si_log(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculate the Scale Invarient error that takes into account the global scale of a scene. \n",
    "        This metric is sensitive to the relationships between points in the scene, \n",
    "        irrespective of the absolute global scale.\n",
    "\n",
    "        Parameters:\n",
    "        y_pred : torch.Tensor\n",
    "            Predicted depth values.\n",
    "        y_true : torch.Tensor\n",
    "            Ground truth depth values.\n",
    "    \n",
    "        Returns:\n",
    "        float\n",
    "            SI Error\n",
    "            \n",
    "        \"\"\"\n",
    "        bs = y_pred.shape[0]\n",
    "\n",
    "        y_pred = torch.reshape(y_pred, (bs, -1))\n",
    "        y_true = torch.reshape(y_true, (bs, -1))\n",
    "\n",
    "        mask = y_true > 0  # 0=missing y_true\n",
    "        num_vals = mask.numel()\n",
    "\n",
    "        log_diff = torch.zeros_like(y_pred)\n",
    "        log_diff[mask] = torch.log(y_pred[mask]) - torch.log(y_true[mask])\n",
    "        \n",
    "        si_log_unscaled = torch.sum(log_diff**2, dim=1) / num_vals - (torch.sum(log_diff, dim=1)**2) / (num_vals**2)\n",
    "        si_log_score = torch.sqrt(si_log_unscaled) * 100\n",
    "        \n",
    "        si_log_score = torch.mean(si_log_score)\n",
    "        return si_log_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now get the depths for the whole dataset, and compute baseline results for MAE and delta1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthEstimationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, label_dir, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = []\n",
    "        self.mask_files = []\n",
    "        \n",
    "        for root, _, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    image_file = os.path.join(root, file)\n",
    "                    self.image_files.append(image_file)\n",
    "                    \n",
    "        for root, _, files in os.walk(label_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    mask_file = os.path.join(root, file)\n",
    "                    self.mask_files.append(mask_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.image_files[index]\n",
    "        mask_file = self.mask_files[index]\n",
    "        \n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        mask = Image.open(mask_file)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((1080, 1920))\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the data\n",
    "train_data = DepthEstimationDataset('eiffel/2020/images/','eiffel/2020/depth/dense/depth', transforms=data_transforms)\n",
    "# test_data = SegmentationDataset('eiffel/2020/depth/dense/depth', transforms=data_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/main/en/preprocessing#computer-vision\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    absRel = 0\n",
    "    rmse = 0\n",
    "    rmseLog = 0\n",
    "    delta1 = 0\n",
    "    si_error = 0\n",
    "    total = 0\n",
    "    # Wrap the dataloader with tqdm for progress tracking\n",
    "    dataloader = tqdm(dataloader, total=num_batches, desc=\"Evaluation\")\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        inputs = image_processor(images=data, return_tensors=\"pt\", do_rescale= False).to(device)\n",
    "        \n",
    "        \n",
    "        # no training therefore no calculation of gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "            # interpolate to original size\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                predicted_depth.unsqueeze(1),\n",
    "                size=[data.shape[2], data.shape[3]],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            # Convert depth prediction to numpy array and resize to match ground truth depth map size\n",
    "            depth_output = prediction.squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            # Handle invalid or unexpected depth values\n",
    "            mask = labels == 0. \n",
    "            depth_output[mask] = 0.  # Replace negative or zero values with a small epsilon \n",
    "            \n",
    "            # Calculate metrics\n",
    "            absRel += np.sum(absolute_relative_error(depth_output, labels))\n",
    "            \n",
    "            rmse += np.sum(root_mean_squared_error(depth_output, labels))\n",
    "            rmseLog += np.sum(root_mean_squared_error(depth_output, labels, log = True))\n",
    "            \n",
    "            out_t = torch.from_numpy(depth_output)\n",
    "            labels_t = torch.from_numpy(labels)\n",
    "            \n",
    "            delta1 += np.sum(delta1_metric(out_t, labels_t))\n",
    "            # si_error += si_log(out_t, labels_t)\n",
    "\n",
    "            total += data.size(0)\n",
    "            # Update tqdm progress bar\n",
    "            dataloader.set_postfix({'absRel': absRel/total, 'RMSE': rmse/total, 'rmseLog': rmseLog/total,\n",
    "                                    'delta':delta1/total, 'siLog': si_error/total})\n",
    "        \n",
    "    total_absRel = absRel/total\n",
    "    total_rmse = rmse/total\n",
    "    total_rmseLog = rmseLog/total\n",
    "    total_delta = delta1/total\n",
    "    total_si = si_error/total\n",
    "    \n",
    "    return total_absRel, total_rmse, total_rmseLog, total_delta, total_si\n",
    "    \n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")   \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "result = evaluate_model(model, train_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta1 is better when it is close to one.\n",
    "MAE and RMSE are better when they are close to 0. \n",
    "We have a good RMSE, an average AbsRel and a poor Delta1. \n",
    "What can we say about these baselines? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-large-hf\")   \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "result_large = evaluate_model(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas to use pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "small = (\"DepthAnything-Small\", result[0], result[1], result[2], result[3])\n",
    "large = (\"DepthAnything-Large\", result_large[0], result_large[1], result_large[2], result_large[3])\n",
    "\n",
    "# data in the form of list of tuples\n",
    "data = [small, large]\n",
    " \n",
    "# create DataFrame using data\n",
    "df = pd.DataFrame(data, columns =['Model', 'AbsRel', 'RMSE', 'RMSE Log', 'Delta'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python PEFT_training/run_training.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "image, label = train_data[5]\n",
    "label = label.numpy().astype(np.uint8)\n",
    "label = label.transpose(1, 2, 0)  # Convert to shape (1080, 1920, 3)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(label, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges_original = cv2.Canny(label, threshold1=300, threshold2=450)  # Adjust thresholds as needed\n",
    "# Apply Gaussian blur to the detected edges\n",
    "blurred_edges = cv2.GaussianBlur(edges_original, (5, 5), 0) \n",
    "inputs = image_processor(images=image, return_tensors=\"pt\", do_rescale = False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "# Interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=[np.array(image).shape[1], np.array(image).shape[2]],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# Convert depth prediction to numpy array\n",
    "depth_output = prediction.squeeze().cpu().numpy()\n",
    "# # Visualize the results\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].imshow(edges_original, cmap='gray')\n",
    "axes[0].set_title('Detected edges on label')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(label, cmap='plasma_r')\n",
    "axes[1].set_title('Label')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(blurred_edges, cmap='gray')\n",
    "axes[2].set_title('Blurred Edges Label')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(depth_output, cmap='plasma_r')\n",
    "axes[3].set_title('DepthAnything output')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(depth_output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(depth_output, blurred_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we get an improved result. Let's try this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_labels(batch_labels):\n",
    "    preprocessed_batch = []\n",
    "    \n",
    "    for labels in batch_labels:\n",
    "        # Prepare labels\n",
    "        labels = labels.numpy().astype(np.uint8)\n",
    "        labels = labels.transpose(1, 2, 0)\n",
    "        \n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(labels, threshold1=300, threshold2=450)  # Adjust thresholds as needed\n",
    "        \n",
    "        # Apply Gaussian blur to the detected edges\n",
    "        blurred = cv2.GaussianBlur(edges, (5, 5), 0)\n",
    "        \n",
    "        # Add the preprocessed image to the batch list\n",
    "        preprocessed_batch.append(blurred)\n",
    "\n",
    "    return torch.Tensor(np.array(preprocessed_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/main/en/preprocessing#computer-vision\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    absRel = 0\n",
    "    rmse = 0\n",
    "    rmseLog = 0\n",
    "    delta1 = 0\n",
    "    si_error = 0\n",
    "    total = 0\n",
    "    # Wrap the dataloader with tqdm for progress tracking\n",
    "    dataloader = tqdm(dataloader, total=num_batches, desc=\"Evaluation\")\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        labels = preprocess_labels(labels)\n",
    "        inputs = image_processor(images=data, return_tensors=\"pt\", do_rescale= False).to(device)\n",
    "        \n",
    "        # no training therefore no calculation of gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "            # interpolate to original size\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                predicted_depth.unsqueeze(1),\n",
    "                size=[data.shape[2], data.shape[3]],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            # Convert depth prediction to numpy array and resize to match ground truth depth map size\n",
    "            depth_output = prediction.squeeze().cpu().numpy()\n",
    "            labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "            # Handle invalid or unexpected depth values\n",
    "            mask = labels == 0. \n",
    "            depth_output[mask] = 0.  # Replace negative or zero values with a small epsilon \n",
    "            \n",
    "            # Calculate metrics\n",
    "            absRel += np.sum(absolute_relative_error(depth_output, labels))\n",
    "            \n",
    "            rmse += np.sum(root_mean_squared_error(depth_output, labels))\n",
    "            rmseLog += np.sum(root_mean_squared_error(depth_output, labels, log = True))\n",
    "            \n",
    "            out_t = torch.from_numpy(depth_output)\n",
    "            labels_t = torch.from_numpy(labels)\n",
    "            \n",
    "            delta1 += np.sum(delta1_metric(out_t, labels_t))\n",
    "            si_error += si_log(out_t, labels_t)\n",
    "\n",
    "            total += data.size(0)\n",
    "            # Update tqdm progress bar\n",
    "            dataloader.set_postfix({'absRel': absRel/total, 'RMSE': rmse/total, 'rmseLog': rmseLog/total,\n",
    "                                    'delta':delta1/total, 'siLog': si_error/total})\n",
    "        \n",
    "    total_absRel = absRel/total\n",
    "    total_rmse = rmse/total\n",
    "    total_rmseLog = rmseLog/total\n",
    "    total_delta = delta1/total\n",
    "    total_si = si_error/total\n",
    "    \n",
    "    return total_absRel, total_rmse, total_rmseLog, total_delta, total_si\n",
    "    \n",
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")   \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "result = evaluate_model(model, train_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-large-hf\")   \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model.to(device)\n",
    "result_large = evaluate_model(model, train_loader)\n",
    "result_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training again with the changes -> experiment 2, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset: 1610\n",
      "trainable params: 599040 || all params: 25374913 || trainable%: 2.36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcamilla-james\u001b[0m (\u001b[33mdolphins\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/mundus/cjames706/underwater_depth_estimation/wandb/run-20240525_201611-2q439v0d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTesting with 20 epochs, EiffelTowerDataset lr: 0.0001, warmup: 40, optim: AdamW\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/dolphins/DepthUnderwater_training\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dolphins/DepthUnderwater_training/runs/2q439v0d\u001b[0m\n",
      "EPOCH 1:\n",
      "  batch 12 loss: 4.816017309824626\n",
      "  batch 24 loss: 5.114706873893738\n",
      "  batch 36 loss: 4.827160080273946\n",
      "  batch 48 loss: 4.774953166643779\n",
      "  batch 60 loss: 4.631075739860535\n",
      "  batch 72 loss: 4.360792100429535\n",
      "  batch 84 loss: 4.0735628207524615\n",
      "  batch 96 loss: 3.9591227968533835\n",
      "  batch 108 loss: 3.5435516238212585\n",
      "LOSS train 3.5435516238212585 valid 3.0782320499420166\n",
      "EPOCH 2:\n",
      "  batch 12 loss: 3.141525129477183\n",
      "  batch 24 loss: 2.7175680796305337\n",
      "  batch 36 loss: 2.150677661101023\n",
      "  batch 48 loss: 1.6340686182181041\n",
      "  batch 60 loss: 1.4115593036015828\n",
      "  batch 72 loss: 1.0080316414435704\n",
      "  batch 84 loss: 0.6892626310388247\n",
      "  batch 96 loss: 0.45165402193864185\n",
      "  batch 108 loss: 0.33381952221194905\n",
      "LOSS train 0.33381952221194905 valid 0.3361510932445526\n",
      "EPOCH 3:\n",
      "  batch 12 loss: 0.2534386081000169\n",
      "  batch 24 loss: 0.22957225516438484\n",
      "  batch 36 loss: 0.21622392411033312\n",
      "  batch 48 loss: 0.18914789830644926\n",
      "  batch 60 loss: 0.1778878172238668\n",
      "  batch 72 loss: 0.16989342495799065\n",
      "  batch 84 loss: 0.14641631146272024\n",
      "  batch 96 loss: 0.14085914256672064\n",
      "  batch 108 loss: 0.13379433875282606\n",
      "LOSS train 0.13379433875282606 valid 0.15283368527889252\n",
      "EPOCH 4:\n",
      "  batch 12 loss: 0.15260455757379532\n",
      "  batch 24 loss: 0.12227652656535308\n",
      "  batch 36 loss: 0.11577228022118409\n",
      "  batch 48 loss: 0.11366056961317857\n",
      "  batch 60 loss: 0.11608665312329929\n",
      "  batch 72 loss: 0.1062628695120414\n",
      "  batch 84 loss: 0.10079620530207951\n",
      "  batch 96 loss: 0.1026748785128196\n",
      "  batch 108 loss: 0.09190015681087971\n",
      "/home/mundus/cjames706/underwater_depth_estimation/PEFT_training/Evaluation.py:67: RuntimeWarning: invalid value encountered in log\n",
      "  y_true = np.log(y_true)\n",
      "LOSS train 0.09190015681087971 valid 0.11542268097400665\n",
      "EPOCH 5:\n",
      "  batch 12 loss: 0.09499584324657917\n",
      "  batch 24 loss: 0.09824029480417569\n",
      "  batch 36 loss: 0.0923798034588496\n",
      "  batch 48 loss: 0.09502245175341766\n",
      "  batch 60 loss: 0.0889583236227433\n",
      "  batch 72 loss: 0.08760322382052739\n",
      "  batch 84 loss: 0.0835449422399203\n",
      "  batch 96 loss: 0.08521053567528725\n",
      "  batch 108 loss: 0.08200106676667929\n",
      "LOSS train 0.08200106676667929 valid 0.10181877762079239\n",
      "EPOCH 6:\n",
      "  batch 12 loss: 0.08672228765984376\n",
      "  batch 24 loss: 0.08711708585421245\n",
      "  batch 36 loss: 0.08220810318986575\n",
      "  batch 48 loss: 0.08001991671820481\n",
      "  batch 60 loss: 0.09430189058184624\n",
      "  batch 72 loss: 0.08327638699362676\n",
      "  batch 84 loss: 0.0793299488723278\n",
      "  batch 96 loss: 0.08314283813039462\n",
      "  batch 108 loss: 0.08187059623499711\n",
      "LOSS train 0.08187059623499711 valid 0.09823723137378693\n",
      "EPOCH 7:\n",
      "  batch 12 loss: 0.0752556457494696\n",
      "  batch 24 loss: 0.08283259905874729\n",
      "  batch 36 loss: 0.07583594135940075\n",
      "  batch 48 loss: 0.08700097228089969\n",
      "  batch 60 loss: 0.08124009085198243\n",
      "  batch 72 loss: 0.0758288564781348\n",
      "  batch 84 loss: 0.07365585925678413\n",
      "  batch 96 loss: 0.07122308171043794\n",
      "  batch 108 loss: 0.07831559982150793\n",
      "LOSS train 0.07831559982150793 valid 0.11552247405052185\n",
      "EPOCH 8:\n",
      "  batch 12 loss: 0.07543664860228698\n",
      "  batch 24 loss: 0.07171648523459832\n",
      "  batch 36 loss: 0.07812957248340051\n",
      "  batch 48 loss: 0.07273157717039187\n",
      "  batch 60 loss: 0.07142898129920165\n",
      "  batch 72 loss: 0.06818884269644816\n",
      "  batch 84 loss: 0.0745575875043869\n",
      "  batch 96 loss: 0.07150488315771024\n",
      "  batch 108 loss: 0.07429871087272961\n",
      "LOSS train 0.07429871087272961 valid 0.10143512487411499\n",
      "EPOCH 9:\n",
      "  batch 12 loss: 0.0766225674500068\n",
      "  batch 24 loss: 0.07257539437462886\n",
      "  batch 36 loss: 0.06960270305474599\n",
      "  batch 48 loss: 0.0730044146378835\n",
      "  batch 60 loss: 0.06892501655966043\n",
      "  batch 72 loss: 0.06600732325265805\n",
      "  batch 84 loss: 0.06715079055478175\n",
      "  batch 96 loss: 0.06807795073837042\n",
      "  batch 108 loss: 0.0705836396664381\n",
      "LOSS train 0.0705836396664381 valid 0.08570431917905807\n",
      "EPOCH 10:\n",
      "  batch 12 loss: 0.07095402168730895\n",
      "  batch 24 loss: 0.07021770533174276\n",
      "  batch 36 loss: 0.06968187509725492\n",
      "  batch 48 loss: 0.06978447766353686\n",
      "  batch 60 loss: 0.06401734488705794\n",
      "  batch 72 loss: 0.06726236641407013\n",
      "  batch 84 loss: 0.06977344490587711\n",
      "  batch 96 loss: 0.0677171985929211\n",
      "  batch 108 loss: 0.06923359570403893\n",
      "LOSS train 0.06923359570403893 valid 0.09022445976734161\n",
      "EPOCH 11:\n",
      "  batch 12 loss: 0.06623367064942916\n",
      "  batch 24 loss: 0.06470428469280402\n",
      "  batch 36 loss: 0.06153583681831757\n",
      "  batch 48 loss: 0.07073435342560212\n",
      "  batch 60 loss: 0.06645712815225124\n",
      "  batch 72 loss: 0.0711756491412719\n",
      "  batch 84 loss: 0.06623269896954298\n",
      "  batch 96 loss: 0.06854151313503583\n",
      "  batch 108 loss: 0.060377574836214386\n",
      "LOSS train 0.060377574836214386 valid 0.09309583902359009\n",
      "EPOCH 12:\n",
      "  batch 12 loss: 0.06521794013679028\n",
      "  batch 24 loss: 0.06636547017842531\n",
      "  batch 36 loss: 0.06458185706287622\n",
      "  batch 48 loss: 0.06506567168980837\n",
      "  batch 60 loss: 0.06543825784077247\n",
      "  batch 72 loss: 0.06413220955679814\n",
      "  batch 84 loss: 0.06536831675718228\n",
      "  batch 96 loss: 0.06225717067718506\n",
      "  batch 108 loss: 0.06436323467642069\n",
      "LOSS train 0.06436323467642069 valid 0.09017198532819748\n",
      "EPOCH 13:\n",
      "  batch 12 loss: 0.06620400150616963\n",
      "  batch 24 loss: 0.06705479354908069\n",
      "  batch 36 loss: 0.06498148354391257\n",
      "  batch 48 loss: 0.06301343937714894\n",
      "  batch 60 loss: 0.06076712844272455\n",
      "  batch 72 loss: 0.06225506899257501\n",
      "  batch 84 loss: 0.06065896308670441\n",
      "  batch 96 loss: 0.06427475747962792\n",
      "  batch 108 loss: 0.06513084936887026\n",
      "LOSS train 0.06513084936887026 valid 0.08590735495090485\n",
      "EPOCH 14:\n",
      "  batch 12 loss: 0.06341029610484838\n",
      "  batch 24 loss: 0.06726110943903525\n",
      "  batch 36 loss: 0.06167588848620653\n",
      "  batch 48 loss: 0.06542407845457394\n",
      "  batch 60 loss: 0.06246922009934982\n",
      "  batch 72 loss: 0.06355651716391246\n",
      "  batch 84 loss: 0.06450451041261356\n",
      "  batch 96 loss: 0.058014197585483394\n",
      "  batch 108 loss: 0.06456560796747605\n",
      "LOSS train 0.06456560796747605 valid 0.07765026390552521\n",
      "EPOCH 15:\n",
      "  batch 12 loss: 0.057350647946198784\n",
      "  batch 24 loss: 0.06033973333736261\n",
      "  batch 36 loss: 0.06105951716502508\n",
      "  batch 48 loss: 0.06585183429221313\n",
      "  batch 60 loss: 0.06315279100090265\n",
      "  batch 72 loss: 0.067065404728055\n",
      "  batch 84 loss: 0.05935932726909717\n",
      "  batch 96 loss: 0.06029936205595732\n",
      "  batch 108 loss: 0.06735226232558489\n",
      "LOSS train 0.06735226232558489 valid 0.080425925552845\n",
      "EPOCH 16:\n",
      "  batch 12 loss: 0.06402925215661526\n",
      "  batch 24 loss: 0.06445121609916289\n",
      "  batch 36 loss: 0.06184269767254591\n",
      "  batch 48 loss: 0.060364761389791965\n",
      "  batch 60 loss: 0.0635141155992945\n",
      "  batch 72 loss: 0.06292861079176267\n",
      "  batch 84 loss: 0.0608871898924311\n",
      "  batch 96 loss: 0.05666312171767155\n",
      "  batch 108 loss: 0.06393592214832704\n",
      "LOSS train 0.06393592214832704 valid 0.07759206742048264\n",
      "EPOCH 17:\n",
      "  batch 12 loss: 0.06238677011181911\n",
      "  batch 24 loss: 0.0634499875207742\n",
      "  batch 36 loss: 0.06189032644033432\n",
      "  batch 48 loss: 0.060100359842181206\n",
      "  batch 60 loss: 0.06048088210324446\n",
      "  batch 72 loss: 0.05896298121660948\n",
      "  batch 84 loss: 0.0587657413755854\n",
      "  batch 96 loss: 0.06743177957832813\n",
      "  batch 108 loss: 0.06268988984326522\n",
      "LOSS train 0.06268988984326522 valid 0.07787878811359406\n",
      "EPOCH 18:\n",
      "  batch 12 loss: 0.05992818810045719\n",
      "  batch 24 loss: 0.0589045329640309\n",
      "  batch 36 loss: 0.0607574979464213\n",
      "  batch 48 loss: 0.058767683493594326\n",
      "  batch 60 loss: 0.0630805582428972\n",
      "  batch 72 loss: 0.06098865158855915\n",
      "  batch 84 loss: 0.061530064791440964\n",
      "  batch 96 loss: 0.06271153719474871\n",
      "  batch 108 loss: 0.06345848739147186\n",
      "LOSS train 0.06345848739147186 valid 0.07892373204231262\n",
      "EPOCH 19:\n",
      "  batch 12 loss: 0.059425897585848965\n",
      "  batch 24 loss: 0.058158312613765396\n",
      "  batch 36 loss: 0.06108048806587855\n",
      "  batch 48 loss: 0.06148868643989166\n",
      "  batch 60 loss: 0.05765139373640219\n",
      "  batch 72 loss: 0.06094823715587457\n",
      "  batch 84 loss: 0.060262382651368775\n",
      "  batch 96 loss: 0.06536303119113047\n",
      "  batch 108 loss: 0.06463207583874464\n",
      "LOSS train 0.06463207583874464 valid 0.07966236025094986\n",
      "EPOCH 20:\n",
      "  batch 12 loss: 0.06199977764238914\n",
      "  batch 24 loss: 0.06299371334413688\n",
      "  batch 36 loss: 0.060838473960757256\n",
      "  batch 48 loss: 0.059941147143642105\n",
      "  batch 60 loss: 0.059779927134513855\n",
      "  batch 72 loss: 0.059531981125473976\n",
      "  batch 84 loss: 0.06383693652848403\n",
      "  batch 96 loss: 0.05958015378564596\n",
      "  batch 108 loss: 0.05776058665166298\n",
      "LOSS train 0.05776058665166298 valid 0.07894551008939743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Absolute Relative error (AbsRel) ‚ñà‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Delta1 with thresold=1.25 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Learning Rate (per batch) ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Log Root Mean Squared Error (Log-RMSE) ‚ñà‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 Loss/train (per batch) ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         Root Mean Squared Error (RMSE) ‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Scale Invarient Error (SI Error) ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          Training Loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        Validation Loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Absolute Relative error (AbsRel) 0.07703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Delta1 with thresold=1.25 99.99378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              Learning Rate (per batch) 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Log Root Mean Squared Error (Log-RMSE) 0.08028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 Loss/train (per batch) 0.05776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         Root Mean Squared Error (RMSE) 0.04977\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       Scale Invarient Error (SI Error) 186.89285\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          Training Loss 0.05776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        Validation Loss 0.07895\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mTesting with 20 epochs, EiffelTowerDataset lr: 0.0001, warmup: 40, optim: AdamW\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/dolphins/DepthUnderwater_training/runs/2q439v0d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/dolphins/DepthUnderwater_training\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 9 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240525_201611-2q439v0d/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python PEFT_training/run_training.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
